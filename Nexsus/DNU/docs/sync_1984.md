# Data Sync Improvements - Staged Implementation Plan (sync_1984)

## Overview

Implement improvements in stages, testing each before proceeding to the next.

**Command:** `transfer_[model.name]_1984`

---

## Stage 1: Concurrent Sync Prevention

### Goal
Prevent multiple syncs of the same model from running simultaneously.

### Changes

**File: `src/services/data-sync.ts`**

```typescript
// Add at module level (after imports)
const activeSyncs = new Map<string, { startTime: number; progress: number; totalRecords: number }>();

// Export functions for status checking
export function isSyncInProgress(modelName: string): boolean {
  return activeSyncs.has(modelName);
}

export function getActiveSyncs(): Map<string, { startTime: number; progress: number; totalRecords: number }> {
  return new Map(activeSyncs);
}

// In syncModelData() - add at the very beginning (after startTime):
if (activeSyncs.has(config.model_name)) {
  const existing = activeSyncs.get(config.model_name)!;
  const elapsed = Math.round((Date.now() - existing.startTime) / 1000);
  return {
    success: false,
    model_name: config.model_name,
    records_processed: 0,
    records_embedded: 0,
    records_failed: 0,
    duration_ms: 0,
    errors: [
      `Sync already in progress for ${config.model_name}`,
      `Started: ${elapsed}s ago`,
      `Progress: ${existing.progress}% (${Math.round(existing.totalRecords * existing.progress / 100)}/${existing.totalRecords} records)`,
    ],
    restricted_fields: [],
    warnings: [],
  };
}

// Acquire lock immediately after
activeSyncs.set(config.model_name, { startTime: Date.now(), progress: 0, totalRecords: 0 });

// Wrap ALL sync logic in try/finally to ensure lock release
try {
  // ... existing sync logic ...

  // Update progress periodically (in the while loop, after embedding):
  activeSyncs.set(config.model_name, {
    startTime,
    progress: Math.round((totalEmbedded / totalRecords) * 100),
    totalRecords,
  });

} finally {
  // Always release lock, even on error
  activeSyncs.delete(config.model_name);
}
```

### Test Scenarios

| Test | Steps | Expected Result |
|------|-------|-----------------|
| **T1.1: Single sync** | Run `transfer_res.partner_1984` | Sync runs normally |
| **T1.2: Concurrent block** | Start sync, then immediately try another | Second sync rejected with "already in progress" message |
| **T1.3: Progress reporting** | Start sync, try another after 30s | Shows elapsed time and progress % |
| **T1.4: Lock release on error** | Force an error during sync, then retry | Lock released, retry works |
| **T1.5: Lock release on success** | Complete a sync, then start another | Second sync starts normally |

### Verification Commands

```bash
# Test 1: Normal sync
transfer_res.partner_1984 (with test_limit: 10)

# Test 2: While sync running, try again
transfer_res.partner_1984  # Should fail with "already in progress"

# Test 3: After sync completes
transfer_res.partner_1984  # Should start new sync
```

---

## Stage 2: Increase Fetch Batch Size

### Goal
Reduce Odoo API calls by fetching more records per call.

### Changes

**File: `src/constants.ts`**

```typescript
export const DATA_TRANSFORM_CONFIG = {
  FETCH_BATCH_SIZE: 500,  // Was 200 - more records per Odoo call
  EMBED_BATCH_SIZE: 50,   // Keep at 50 for Voyage token limits
  // ... rest unchanged
}
```

### Test Scenarios

| Test | Steps | Expected Result |
|------|-------|-----------------|
| **T2.1: Batch size** | Run sync, check logs | Should show "Fetched batch: 500/X" |
| **T2.2: No timeout** | Sync 1000 records | No "timeout" errors with 500 record batches |
| **T2.3: Speed improvement** | Time full sync | Should be ~20-30% faster than before |

### Verification

```bash
# Look for in logs:
[DataSync] Fetched batch: 500/74952 records (1%)
```

---

## Stage 3: Async Pipeline (Fetch + Embed Overlap)

### Goal
Fetch batch N+1 while embedding batch N to hide latency.

### Changes

**File: `src/services/data-sync.ts`**

Replace the sequential while loop with async pipeline:

```typescript
// Phase 3: STREAMING with async pipeline
const fetchBatchSize = DATA_TRANSFORM_CONFIG.FETCH_BATCH_SIZE;
const embedBatchSize = DATA_TRANSFORM_CONFIG.EMBED_BATCH_SIZE;

// ... totalRecords calculation ...

let offset = 0;
let totalProcessed = 0;
let totalEmbedded = 0;
const maxRecords = config.test_limit || totalRecords;

// Start first fetch
let nextFetchPromise: Promise<{ records: Record<string, unknown>[]; restrictedFields: string[] }> | null = null;

if (offset < maxRecords) {
  const limit = Math.min(fetchBatchSize, maxRecords - offset);
  nextFetchPromise = fetchBatchWithRetry(client, config, domain, currentFieldsToFetch, limit, offset, context, restrictedFieldsMap, warnings);
}

while (nextFetchPromise) {
  // Wait for current batch
  const batchResult = await nextFetchPromise;
  const currentOffset = offset;
  offset += fetchBatchSize;

  // Start fetching NEXT batch immediately (don't wait for embed)
  if (offset < maxRecords) {
    const limit = Math.min(fetchBatchSize, maxRecords - offset);
    nextFetchPromise = fetchBatchWithRetry(client, config, domain, currentFieldsToFetch, limit, offset, context, restrictedFieldsMap, warnings);
  } else {
    nextFetchPromise = null;
  }

  // Process current batch (encode, embed, upsert)
  // ... existing embed logic ...

  // Update progress
  activeSyncs.set(config.model_name, { startTime, progress: Math.round((totalEmbedded / totalRecords) * 100), totalRecords });
}

// Helper function
async function fetchBatchWithRetry(...) {
  return client.searchReadWithRetry<Record<string, unknown>>(...);
}
```

### Test Scenarios

| Test | Steps | Expected Result |
|------|-------|-----------------|
| **T3.1: Overlap verification** | Add timing logs | Fetch N+1 starts before Embed N completes |
| **T3.2: Speed improvement** | Time 1000 records | 30-40% faster than Stage 2 |
| **T3.3: Error handling** | Force fetch error | Gracefully handles, doesn't break pipeline |
| **T3.4: Last batch** | Sync to completion | No hanging promises, clean exit |

### Verification

```bash
# Timing logs should show overlap:
[00:01.000] Fetching batch 1...
[00:02.500] Fetched batch 1, starting embed
[00:02.501] Fetching batch 2...  # Started BEFORE embed 1 finished
[00:05.000] Embedded batch 1
[00:05.500] Fetched batch 2, starting embed
```

---

## Stage 4: Incremental Sync

### Goal
Only sync new/updated records after initial full sync.

### Changes

**New File: `src/services/sync-metadata.ts`**

```typescript
import { readFileSync, writeFileSync, existsSync } from 'fs';
import { join } from 'path';

interface ModelSyncMetadata {
  last_sync_timestamp: string;  // ISO date of last successful sync
  last_max_write_date: string;  // Highest write_date seen
  records_synced: number;
  sync_duration_ms: number;
}

interface SyncMetadataStore {
  models: Record<string, ModelSyncMetadata>;
  version: number;
}

const METADATA_FILE = join(process.cwd(), 'data', 'sync_metadata.json');

export function getLastSyncTimestamp(modelName: string): string | null {
  if (!existsSync(METADATA_FILE)) return null;
  try {
    const data: SyncMetadataStore = JSON.parse(readFileSync(METADATA_FILE, 'utf-8'));
    return data.models[modelName]?.last_max_write_date || null;
  } catch {
    return null;
  }
}

export function saveSyncMetadata(
  modelName: string,
  maxWriteDate: string,
  recordCount: number,
  durationMs: number
): void {
  let data: SyncMetadataStore = { models: {}, version: 1 };

  if (existsSync(METADATA_FILE)) {
    try {
      data = JSON.parse(readFileSync(METADATA_FILE, 'utf-8'));
    } catch {
      // Start fresh if corrupt
    }
  }

  data.models[modelName] = {
    last_sync_timestamp: new Date().toISOString(),
    last_max_write_date: maxWriteDate,
    records_synced: recordCount,
    sync_duration_ms: durationMs,
  };

  writeFileSync(METADATA_FILE, JSON.stringify(data, null, 2));
  console.error(`[SyncMetadata] Saved: ${modelName} last_write_date=${maxWriteDate}`);
}

export function clearSyncMetadata(modelName: string): void {
  if (!existsSync(METADATA_FILE)) return;
  const data: SyncMetadataStore = JSON.parse(readFileSync(METADATA_FILE, 'utf-8'));
  delete data.models[modelName];
  writeFileSync(METADATA_FILE, JSON.stringify(data, null, 2));
}

export function getSyncMetadata(modelName: string): ModelSyncMetadata | null {
  if (!existsSync(METADATA_FILE)) return null;
  const data: SyncMetadataStore = JSON.parse(readFileSync(METADATA_FILE, 'utf-8'));
  return data.models[modelName] || null;
}
```

**File: `src/types.ts`** - Add to DataTransformConfig:

```typescript
export interface DataTransformConfig {
  // ... existing fields
  incremental?: boolean;  // If true, only sync records modified since last sync
  force_full?: boolean;   // If true, ignore last sync timestamp
}
```

**File: `src/services/data-sync.ts`** - Modify domain building:

```typescript
import { getLastSyncTimestamp, saveSyncMetadata } from './sync-metadata.js';

// In syncModelData(), after domain initialization:
let domain: unknown[] = [];
let isIncremental = false;

if (config.incremental && !config.force_full) {
  const lastSync = getLastSyncTimestamp(config.model_name);
  if (lastSync) {
    domain = [['write_date', '>', lastSync]];
    isIncremental = true;
    console.error(`[DataSync] INCREMENTAL sync: fetching records modified after ${lastSync}`);
  } else {
    console.error(`[DataSync] No previous sync found, running FULL sync`);
  }
}

// After successful sync (before return):
if (totalEmbedded > 0) {
  // Find max write_date from processed records
  const maxWriteDate = findMaxWriteDate(allProcessedRecords);
  saveSyncMetadata(config.model_name, maxWriteDate, totalEmbedded, Date.now() - startTime);
}

// Add result field
return {
  // ... existing fields
  sync_type: isIncremental ? 'incremental' : 'full',
};
```

**File: `src/tools/data-tool.ts`** - Add incremental option:

```typescript
// In TransformDataSchema, add:
incremental: z.boolean().optional().describe('If true, only sync new/updated records since last sync'),
force_full: z.boolean().optional().describe('If true, force full sync even if incremental'),

// In tool handler:
const config = {
  // ... existing
  incremental: input.incremental ?? true,  // Default to incremental
  force_full: input.force_full ?? false,
};
```

### Test Scenarios

| Test | Steps | Expected Result |
|------|-------|-----------------|
| **T4.1: First sync (full)** | Run sync on fresh model | Full sync, saves timestamp |
| **T4.2: Incremental** | Modify 1 record in Odoo, run sync | Only 1 record synced |
| **T4.3: No changes** | Run sync without changes | 0 records synced (fast) |
| **T4.4: Force full** | Use force_full=true | Full sync ignoring timestamp |
| **T4.5: Metadata persistence** | Restart server, run sync | Uses saved timestamp |

### Verification

```bash
# First sync (full):
[DataSync] No previous sync found, running FULL sync
[DataSync] Starting streaming sync of 74952 records

# Second sync (incremental):
[DataSync] INCREMENTAL sync: fetching records modified after 2024-12-16T00:45:00Z
[DataSync] Starting streaming sync of 3 records  # Only changed records!

# Check metadata file:
cat data/sync_metadata.json
{
  "models": {
    "res.partner": {
      "last_sync_timestamp": "2024-12-16T01:00:00Z",
      "last_max_write_date": "2024-12-16T00:45:00Z",
      "records_synced": 74952,
      "sync_duration_ms": 3600000
    }
  }
}
```

---

## Stage 5: Deleted Record Cleanup

### Goal
Remove records from vector DB that were deleted in Odoo.

### Changes

**File: `src/services/data-sync.ts`** - Add cleanup function:

```typescript
export async function cleanupDeletedRecords(
  modelName: string,
  modelId: number,
  onProgress?: (current: number, total: number) => void
): Promise<{ deleted: number; errors: string[] }> {
  const client = getOdooClient();
  const errors: string[] = [];

  // 1. Get all active record IDs from Odoo
  const context = { active_test: false };  // Include archived
  const odooRecords = await client.searchRead<{ id: number }>(
    modelName,
    [],
    ['id'],
    { context }
  );
  const odooIds = new Set(odooRecords.map(r => r.id));
  console.error(`[Cleanup] Found ${odooIds.size} records in Odoo`);

  // 2. Get all point IDs from Qdrant for this model
  const qdrantClient = getQdrantClient();
  const vectorPoints = await qdrantClient.scroll(QDRANT_CONFIG.COLLECTION, {
    filter: {
      must: [
        { key: 'model_name', match: { value: modelName } },
        { key: 'point_type', match: { value: 'data' } },
      ],
    },
    limit: 100000,  // Adjust based on expected size
    with_payload: ['record_id'],
  });

  const vectorIds = new Set(vectorPoints.points.map(p => p.payload?.record_id as number));
  console.error(`[Cleanup] Found ${vectorIds.size} records in vector DB`);

  // 3. Find IDs in vector but not in Odoo (deleted)
  const deletedIds: number[] = [];
  for (const vectorId of vectorIds) {
    if (!odooIds.has(vectorId)) {
      deletedIds.push(vectorId);
    }
  }

  console.error(`[Cleanup] Found ${deletedIds.length} deleted records to remove`);

  if (deletedIds.length === 0) {
    return { deleted: 0, errors: [] };
  }

  // 4. Delete from Qdrant
  const pointIdsToDelete = deletedIds.map(rid => generateDataPointId(modelId, rid));

  try {
    await qdrantClient.delete(QDRANT_CONFIG.COLLECTION, {
      points: pointIdsToDelete,
    });
    console.error(`[Cleanup] Deleted ${deletedIds.length} stale records from vector DB`);
  } catch (error) {
    errors.push(`Failed to delete: ${error}`);
  }

  return { deleted: deletedIds.length, errors };
}
```

**File: `src/tools/data-tool.ts`** - Add cleanup tool:

```typescript
server.tool(
  'cleanup_deleted',
  `Remove records from vector DB that were deleted in Odoo.

**Usage:** { "model_name": "res.partner" }`,
  { model_name: z.string() },
  async (args) => {
    const result = await cleanupDeletedRecords(args.model_name, modelId);
    return {
      content: [{
        type: 'text',
        text: `Cleanup Complete\nDeleted: ${result.deleted} stale records`,
      }],
    };
  }
);
```

### Test Scenarios

| Test | Steps | Expected Result |
|------|-------|-----------------|
| **T5.1: No deletions** | Run cleanup on synced model | "0 stale records" |
| **T5.2: With deletions** | Delete record in Odoo, run cleanup | Record removed from vector DB |
| **T5.3: Large cleanup** | Delete 100 records, run cleanup | All 100 removed efficiently |

---

## Implementation Order

```
Stage 1: Concurrent Sync Prevention
    ↓ Test & Verify
Stage 2: Increase Fetch Batch Size
    ↓ Test & Verify
Stage 3: Async Pipeline
    ↓ Test & Verify
Stage 4: Incremental Sync
    ↓ Test & Verify
Stage 5: Deleted Record Cleanup
    ↓ Test & Verify
Done!
```

---

## Files Modified Per Stage

| Stage | Files |
|-------|-------|
| 1 | `src/services/data-sync.ts` |
| 2 | `src/constants.ts` |
| 3 | `src/services/data-sync.ts` |
| 4 | `src/services/sync-metadata.ts` (NEW), `src/services/data-sync.ts`, `src/types.ts`, `src/tools/data-tool.ts` |
| 5 | `src/services/data-sync.ts`, `src/tools/data-tool.ts` |

---

## Expected Results After All Stages

| Scenario | Before | After All Stages |
|----------|--------|------------------|
| Full sync (75K records) | 6+ hours | ~2-3 hours |
| Incremental (few changes) | 6+ hours | ~2-3 minutes |
| Concurrent sync attempt | Both run | Second blocked with status |
| Deleted records | Stay forever | Cleaned up on demand |
